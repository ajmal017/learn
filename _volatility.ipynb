{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38132bit2115ca79f6634adbad3a74c57c1d7c04",
   "display_name": "Python 3.8.1 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "Ref: [Calculating centred and non-centred volatility](http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html)\n",
    "\n",
    "Stock Volatility measures how much a stock tends to move. There are many ways to calculate volatility, for e.g. through:\n",
    "\n",
    "- Daily/Weekly/Monthly range\n",
    "- Average True Range\n",
    "- Standard Deviation\n",
    "\n",
    "Standard Deviation is the most popular way to measure volatility. Standard Deviation for stock price can be computed in multiple ways. Let us go through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# computing SD of a series of values\n",
    "data = [161.47, 159.86, 159.27, 159.98, 159.78, 157.21, 157.5, 157.86, 160.95, 161.6, 159.85, \n",
    "         157.48, 155.32, 160.43, 159.45, 158.19, 155.78, 154.96, 156.53, 149.46, 148.15] # gives sd of 1.59%\n",
    "\n",
    "s = pd.Series(data, name='data')\n",
    "df = pd.DataFrame(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) From math library's standard deviation computation.\n",
    "\n",
    "The code for this is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = df.data.std()\n",
    "amean = df.data.mean()\n",
    "dict(asd=asd, amean=amean)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arithmetic standard deviation makes sense for a stable mean. However, in stock prices the price keeps current price keeps changing. So it is better to use **geometric standard deviation** or GSD. \n",
    "\n",
    "Here are different ways of getting GSD:\n",
    "\n",
    "## 2a) From daily log returns assuming zero mean and averaging squares\n",
    "Ref: [this Quora link](https://www.quora.com/How-do-you-calculate-the-standard-deviation-for-a-stock)\n",
    "\n",
    "Formula for geometric standard deviation is represented by:\n",
    "     \n",
    "gsd1 = $\\exp{\\Big(\\sqrt{{\\frac{\\sum_{1}^n (\\ln\\frac{x[n]}{x[n-1]})^{2}}{n}}}}\\Big)$\n",
    "\n",
    "Geometric standard deviation is computed over geometric mean, which is represented by the formula:\n",
    "\n",
    "gm = $\\left (\\prod_{a=1}^{b}x_i  \\right )^{\\frac{1}{n}} = \\sqrt[n]{x_1x_2...x_n}$\n",
    "\n",
    "The upper and lower values of 1 GSD are computed by dividing and multiplying geometric mean with the geometric standard deviation\n",
    "\n",
    "The code for these are as follows:\n",
    "\n",
    "```\n",
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd1 = math.exp(math.sqrt(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1])**2, raw=True).mean()))\n",
    "gm = gmean(df)\n",
    "glo1 = gm*(1-(gm-gm/gsd1)/gm*sdmult) # gm/gsd1 ... but taking sdmult into consideration\n",
    "ghi1 = gm*(1+(gm*gsd1-gm)/gm*sdmult) # gm*gsd1 ... but taking sdmult into consideration\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd1 = math.exp(math.sqrt(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1])**2, raw=True).mean()))\n",
    "gm = gmean(df)\n",
    "glo1 = gm*(1-(gm-gm/gsd1)/gm*sdmult) # gm/gsd1 ... but taking sdmult into consideration\n",
    "ghi1 = gm*(1+(gm*gsd1-gm)/gm*sdmult) # gm*gsd1 ... but taking sdmult into consideration\n",
    "\n",
    "dict(gm=gm, gsd1=gsd1, glo1=glo1, ghi1=ghi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2b) From daily log returns without squaring and square-rooting\n",
    "\n",
    "The formula for this is:\n",
    "\n",
    "sd2 = ${\\exp(\\sigma\\Big({{\\ln(\\frac{x[n]}{x[n-1]})}}\\Big))}$\n",
    "\n",
    "This equation gives a slightly lesser band than taking rooting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd2 = math.exp(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1]), raw=True).std())\n",
    "glo2 = gm*(1-(gm-gm/gsd2)/gm*sdmult) # gm/gsd2 ... but taking sdmult into consideration\n",
    "ghi2 = gm*(1+(gm*gsd2-gm)/gm*sdmult) # gm*gsd2 ... but taking sdmult into consideration\n",
    "\n",
    "dict(gm=gm, gsd2=gsd2, glo2=glo2, ghi2=ghi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2c) Centered Historical Volatiity using look-back (Bill Luby)\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Select a desired lookback period in trading days (lookback period)\n",
    "2. Gather closing prices for the full lookback period, plus one additional day (lookback +1)\n",
    "4. Calculate the daily close-to-close price changes in a security for each day in the lookback period (daily change)\n",
    "5. Determine the natural log of each daily percentage change (log of daily changes)\n",
    "6. Calculate the mean of all the natural logs of the closing prices for the lookback period (log lookback mean)\n",
    "7. For each day, subtract the lookback mean from the log of daily changes (daily difference)\n",
    "8. Square all the differences between the mean and the daily change (daily variance)\n",
    "9. Sum all the squares of the differences (sum of variances)\n",
    "10. Divide the sum of the squares of the variances by the lookback period (lookback variance)\n",
    "11. Take the square root of the lookback variance (historical volatility, expressed as a standard deviation)\n",
    "12. To convert this to an annual volatility percent, take HV expressed as standard deviation and multiply it by square root of no of trading days in a year (252) and then by 100.\n",
    "13. Compute the high and low bands for the lookback period\n",
    "\n",
    "## 2d) Non-centered Historical volatility on a lookback (Bill Luby - faster)\n",
    "\n",
    "The previous calculation reflects a centred approach where daily price changes are characterized relative to a mean value for the entire period. Instead of that one could assume that, in the long run, the mean change in price approaches zero and hence not meamingful. So, if the mean is not meaningful, we can dump it and not subtract it from the daily changes, so all calculations involving the mean can be dropped! This non-centred approach for historical volaility is sometimes called \"ditching the mean\".\n",
    "\n",
    "This method is shorter and [gives better numbers to traders.](http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html). \n",
    "\n",
    "In this calculation the following steps are taken:\n",
    "\n",
    "1. Select a desired (10-day) lookback period in trading days (lookback period)\n",
    "2. Gather closing prices for the full lookback period, plus one additional day (lookback +1)\n",
    "4. Calculate the daily close-to-close price changes in a security for each day in the lookback period (daily change)\n",
    "5. Determine the natural log of each daily percentage change (log of daily changes)\n",
    "6. Determine the historical volatility by multiplying the Standard Deviation of the log change with root of 252 and multiplying by 100.\n",
    "\n",
    "Let us now see how all of the above work on a pandas ohlc data-set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "MARKET = 'nse'\n",
    "TRADING_DAYS=252\n",
    "STDMULT = 2\n",
    "\n",
    "oldp = str(Path(os.getcwd()).parents[3])+f\"\\ibkr\\data\\{MARKET.lower()}\"\n",
    "newp = str(Path(os.getcwd()).parents[0])+f\"\\ib\\data\\{MARKET.lower()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ohlc = pd.read_pickle(newp+'\\\\_df_ohlc.pkl')\n",
    "df_und = pd.read_pickle(newp+'\\\\_df_und.pkl')\n",
    "\n",
    "# pick up NIFTY50 only and reset its index\n",
    "cols = ['symbol', 'dte', 'date', 'close', 'rise', 'fall'] # interested columns\n",
    "df = df_all_ohlc[df_all_ohlc.symbol=='NIFTY50']\n",
    "\n",
    "# extract from df_und data\n",
    "df = df.set_index('symbol').join(df_und[['symbol', 'undPrice', 'iv_ib', 'hv_ib', 'avg_div']].set_index('symbol')).reset_index()\n",
    "\n",
    "# replace iv_ib and hv_ib as per dte\n",
    "df = df.assign(\n",
    "    iv_ib=df.iv_ib*[math.sqrt(j/365) for j in df.index],\n",
    "    hv_ib=df.hv_ib*[math.sqrt(j/365) for j in df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the comparisons of various algorithms for hv.\n",
    "\n",
    "### 1) Luby's formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hv_luby(df):\n",
    "    \"\"\"Computes historical volatility based on Bill Luby's formula\n",
    "    Ref: http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html\n",
    "    Arg: df as dataframe for a symbol with close field\n",
    "    Outputs: historical volaility percentage\n",
    "    \"\"\"\n",
    "    # Determine the natural log of each daily percentage change (log of daily changes)\n",
    "    log_of_changes = df.close.rolling(2).apply(\n",
    "        func=lambda x: math.log(x[1]/x[0]),\n",
    "        raw=True)\n",
    "\n",
    "    # Calculate the mean of all the natural logs of the closing prices for the lookback period (log lookback mean)\n",
    "    lookback_mean = log_of_changes.expanding().mean()\n",
    "\n",
    "    # For each day, subtract the lookback mean from the log of daily changes (daily difference)\n",
    "    daily_difference = log_of_changes - lookback_mean\n",
    "\n",
    "    # Square all the differences between the mean and the daily change (daily variance)\n",
    "    daily_variance = daily_difference**2\n",
    "\n",
    "    # Sum all the squares of the differences (sum of variances)\n",
    "    sum_of_variances = daily_variance.expanding().sum()\n",
    "\n",
    "    # Divide the sum of the squares of the variances by the lookback period (lookback variance)\n",
    "    lookback_variance = sum_of_variances/sum_of_variances.index\n",
    "\n",
    "    # Take the square root of the lookback variance (historical volatility, expressed as a standard deviation)\n",
    "    hv_as_sd = lookback_variance.apply(lambda x: math.sqrt(x))\n",
    "\n",
    "    # Compute annual volatility by multiplying by root of 252\n",
    "    hv_luby = hv_as_sd.apply(lambda x: x*math.sqrt(TRADING_DAYS))\n",
    "\n",
    "    df = df.assign(hv_luby=hv_luby)\n",
    "\n",
    "    return df\n",
    "\n",
    "df  = hv_luby(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) TastyTrades formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the natural log of each daily percentage change (log of daily changes)\n",
    "log_of_changes = df.close.rolling(2).apply(\n",
    "    func=lambda x: math.log(x[1]/x[0]),\n",
    "    raw=True)\n",
    "\n",
    "# Calculate the mean of all the natural logs of the closing prices for the lookback period (log lookback mean)\n",
    "lookback_mean = log_of_changes.expanding().mean()\n",
    "\n",
    "# For each day, subtract the lookback mean from the log of daily changes (daily difference)\n",
    "daily_difference = log_of_changes - lookback_mean\n",
    "\n",
    "# Square all the differences between the mean and the daily change (daily variance)\n",
    "daily_variance = daily_difference**2\n",
    "\n",
    "# Sum all the squares of the differences (sum of variances)\n",
    "sum_of_variances = daily_variance.expanding().sum()\n",
    "\n",
    "# Divide the sum of the squares of the variances by the lookback period (lookback variance)\n",
    "lookback_variance = sum_of_variances/sum_of_variances.index\n",
    "\n",
    "# Take the square root of the lookback variance (historical volatility, expressed as a standard deviation)\n",
    "hv_as_sd = lookback_variance.apply(lambda x: math.sqrt(x))\n",
    "\n",
    "# Compute annual volatility by multiplying by root of 252\n",
    "hv_luby = hv_as_sd.apply(lambda x: x*math.sqrt(TRADING_DAYS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_as_sd*math.sqrt(TRADING_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see what the Luby's standard deviation for the entire data set is:\n",
    "\n",
    "def hv_luby_365(df):\n",
    "    \"\"\"Compute Luby's historical volatility for 365 day chunks\n",
    "    Arg: df as dataframe of 365 days\n",
    "    Returns: historical volatilty as a float\"\"\"\n",
    "    df.reset_index(drop=True)\n",
    "    log_of_changes = df.close.rolling(2).apply(lambda x: math.log(x[0]/x[-1]))\n",
    "    lookback_mean = log_of_changes.expanding().mean()\n",
    "    daily_difference = log_of_changes - lookback_mean\n",
    "    daily_variance = daily_difference**2\n",
    "    sum_of_variances = daily_variance.sum()\n",
    "    lookback_variance = sum_of_variances/(len(df)+1)\n",
    "    hv_as_sd = math.sqrt(lookback_variance)\n",
    "    hv_luby = hv_as_sd*math.sqrt(TRADING_DAYS)\n",
    "    return hv_luby\n",
    "\n",
    "# df.apply(lambda x: x[::-1]).apply(hv_luby_365)\n",
    "hv_luby_365(df[:365])\n",
    "\n",
    "dfs = [df[x: x+365] for x in range(0, len(df)-364)]\n",
    "[hv_luby_365(df) for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hv_tw(df):\n",
    "    \"\"\"Historical Volatility by Tasty Works\n",
    "    Ref: https://youtu.be/omVKR85pw2s\n",
    "    Arg: df as dataframe for a symbol with close field\n",
    "    Outputs: historical volaility percentage\n",
    "    Note: To compute price bands\n",
    "    Upper Band = price*(1 + ln_avg + STDMULT*ln_stdev)\n",
    "    Lower Band = price*(1 + ln_avg - STDMULT*ln_stdev)\n",
    "    \"\"\"\n",
    "    # Log-normal of price ratio\n",
    "    ln = df.close.rolling(2).apply(\n",
    "            lambda x: math.log(x[1]/x[0]))\n",
    "\n",
    "    # stdev and mean of ln\n",
    "    ln_avg = ln.expanding().mean()\n",
    "    ln_stdev = ln.expanding().std(ddof=1)\n",
    "\n",
    "    df = df.assign(ln_avg=ln_avg, ln_stdev=ln_stdev)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = hv_tw(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all of them together\n",
    "\n",
    "up_tw = pd.Series(df.undPrice*(1+df.ln_avg+STDMULT*df.ln_stdev), name='up_tw')\n",
    "lo_tw = pd.Series(df.undPrice*(1+df.ln_avg-STDMULT*df.ln_stdev), name='lo_tw')\n",
    "up_luby = pd.Series(df.undPrice*(1+STDMULT*df.hv_luby), name='up_luby')\n",
    "lo_luby = pd.Series(df.undPrice*(1-STDMULT*df.hv_luby), name='lo_luby')\n",
    "up_iv_ib = pd.Series(df.undPrice*(1+STDMULT*df.iv_ib), name='up_iv_ib')\n",
    "lo_iv_ib = pd.Series(df.undPrice*(1-STDMULT*df.iv_ib), name='lo_iv_ib')\n",
    "up_hv_ib = pd.Series(df.undPrice*(1+STDMULT*df.hv_ib), name='up_hv_ib')\n",
    "lo_hv_ib = pd.Series(df.undPrice*(1-STDMULT*df.hv_ib), name='lo_hv_ib')\n",
    "\n",
    "df=df.assign(up_tw=up_tw, lo_tw=lo_tw, up_luby=up_luby, lo_luby=lo_luby, up_iv_ib=up_iv_ib, lo_iv_ib=lo_iv_ib, up_hv_ib=up_hv_ib, lo_hv_ib=lo_hv_ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Note</em>: The following cells are work-in-progress and have been temporarily abandoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>dte</th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>average</th>\n      <th>barCount</th>\n      <th>rise</th>\n      <th>fall</th>\n      <th>undPrice</th>\n      <th>iv_ib</th>\n      <th>hv_ib</th>\n      <th>avg_div</th>\n      <th>gm_fact</th>\n      <th>gsd_fact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NIFTY50</td>\n      <td>0</td>\n      <td>2020-02-18</td>\n      <td>12027.85</td>\n      <td>12028.95</td>\n      <td>11960.50</td>\n      <td>11964.85</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1285</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>11966.049805</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>148.6915</td>\n      <td>0.000100</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NIFTY50</td>\n      <td>1</td>\n      <td>2020-02-17</td>\n      <td>12131.55</td>\n      <td>12159.35</td>\n      <td>12037.20</td>\n      <td>12045.80</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>21269</td>\n      <td>569.40</td>\n      <td>318.45</td>\n      <td>11966.049805</td>\n      <td>0.007247</td>\n      <td>0.007353</td>\n      <td>148.6915</td>\n      <td>-0.003277</td>\n      <td>0.006766</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NIFTY50</td>\n      <td>2</td>\n      <td>2020-02-14</td>\n      <td>12194.70</td>\n      <td>12246.50</td>\n      <td>12091.65</td>\n      <td>12113.45</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>21310</td>\n      <td>895.40</td>\n      <td>541.80</td>\n      <td>11966.049805</td>\n      <td>0.010249</td>\n      <td>0.010398</td>\n      <td>148.6915</td>\n      <td>-0.006281</td>\n      <td>0.006217</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NIFTY50</td>\n      <td>3</td>\n      <td>2020-02-13</td>\n      <td>12219.55</td>\n      <td>12219.70</td>\n      <td>12140.00</td>\n      <td>12174.65</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>21131</td>\n      <td>883.40</td>\n      <td>691.85</td>\n      <td>11966.049805</td>\n      <td>0.012552</td>\n      <td>0.012735</td>\n      <td>148.6915</td>\n      <td>-0.009058</td>\n      <td>0.005855</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NIFTY50</td>\n      <td>4</td>\n      <td>2020-02-12</td>\n      <td>12151.00</td>\n      <td>12231.55</td>\n      <td>12145.05</td>\n      <td>12201.20</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>20989</td>\n      <td>782.60</td>\n      <td>660.25</td>\n      <td>11966.049805</td>\n      <td>0.014494</td>\n      <td>0.014705</td>\n      <td>148.6915</td>\n      <td>-0.011168</td>\n      <td>0.005185</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>486</th>\n      <td>NIFTY50</td>\n      <td>486</td>\n      <td>2018-02-23</td>\n      <td>10405.00</td>\n      <td>10499.00</td>\n      <td>10397.25</td>\n      <td>10491.05</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>20708</td>\n      <td>1822.80</td>\n      <td>1473.80</td>\n      <td>11966.049805</td>\n      <td>0.159765</td>\n      <td>0.162090</td>\n      <td>148.6915</td>\n      <td>0.065673</td>\n      <td>0.008564</td>\n    </tr>\n    <tr>\n      <th>487</th>\n      <td>NIFTY50</td>\n      <td>487</td>\n      <td>2018-02-22</td>\n      <td>10354.35</td>\n      <td>10397.45</td>\n      <td>10341.55</td>\n      <td>10382.70</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>20572</td>\n      <td>1796.25</td>\n      <td>1582.15</td>\n      <td>11966.049805</td>\n      <td>0.159930</td>\n      <td>0.162256</td>\n      <td>148.6915</td>\n      <td>0.065815</td>\n      <td>0.008569</td>\n    </tr>\n    <tr>\n      <th>488</th>\n      <td>NIFTY50</td>\n      <td>488</td>\n      <td>2018-02-21</td>\n      <td>10426.00</td>\n      <td>10426.00</td>\n      <td>10349.80</td>\n      <td>10397.45</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>20628</td>\n      <td>1735.05</td>\n      <td>1567.40</td>\n      <td>11966.049805</td>\n      <td>0.160094</td>\n      <td>0.162423</td>\n      <td>148.6915</td>\n      <td>0.065953</td>\n      <td>0.008560</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>NIFTY50</td>\n      <td>489</td>\n      <td>2018-02-20</td>\n      <td>10391.00</td>\n      <td>10428.70</td>\n      <td>10348.20</td>\n      <td>10360.40</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>20933</td>\n      <td>1667.40</td>\n      <td>1604.45</td>\n      <td>11966.049805</td>\n      <td>0.160258</td>\n      <td>0.162589</td>\n      <td>148.6915</td>\n      <td>0.066098</td>\n      <td>0.008553</td>\n    </tr>\n    <tr>\n      <th>490</th>\n      <td>NIFTY50</td>\n      <td>490</td>\n      <td>2018-02-19</td>\n      <td>10488.20</td>\n      <td>10488.20</td>\n      <td>10303.30</td>\n      <td>10378.40</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>21099</td>\n      <td>1586.45</td>\n      <td>1586.45</td>\n      <td>11966.049805</td>\n      <td>0.160422</td>\n      <td>0.162755</td>\n      <td>148.6915</td>\n      <td>0.066239</td>\n      <td>0.008544</td>\n    </tr>\n  </tbody>\n</table>\n<p>491 rows × 18 columns</p>\n</div>",
      "text/plain": "      symbol  dte       date      open      high       low     close  volume  \\\n0    NIFTY50    0 2020-02-18  12027.85  12028.95  11960.50  11964.85       0   \n1    NIFTY50    1 2020-02-17  12131.55  12159.35  12037.20  12045.80       0   \n2    NIFTY50    2 2020-02-14  12194.70  12246.50  12091.65  12113.45       0   \n3    NIFTY50    3 2020-02-13  12219.55  12219.70  12140.00  12174.65       0   \n4    NIFTY50    4 2020-02-12  12151.00  12231.55  12145.05  12201.20       0   \n..       ...  ...        ...       ...       ...       ...       ...     ...   \n486  NIFTY50  486 2018-02-23  10405.00  10499.00  10397.25  10491.05       0   \n487  NIFTY50  487 2018-02-22  10354.35  10397.45  10341.55  10382.70       0   \n488  NIFTY50  488 2018-02-21  10426.00  10426.00  10349.80  10397.45       0   \n489  NIFTY50  489 2018-02-20  10391.00  10428.70  10348.20  10360.40       0   \n490  NIFTY50  490 2018-02-19  10488.20  10488.20  10303.30  10378.40       0   \n\n     average  barCount     rise     fall      undPrice     iv_ib     hv_ib  \\\n0        0.0      1285     0.00     0.00  11966.049805  0.000000  0.000000   \n1        0.0     21269   569.40   318.45  11966.049805  0.007247  0.007353   \n2        0.0     21310   895.40   541.80  11966.049805  0.010249  0.010398   \n3        0.0     21131   883.40   691.85  11966.049805  0.012552  0.012735   \n4        0.0     20989   782.60   660.25  11966.049805  0.014494  0.014705   \n..       ...       ...      ...      ...           ...       ...       ...   \n486      0.0     20708  1822.80  1473.80  11966.049805  0.159765  0.162090   \n487      0.0     20572  1796.25  1582.15  11966.049805  0.159930  0.162256   \n488      0.0     20628  1735.05  1567.40  11966.049805  0.160094  0.162423   \n489      0.0     20933  1667.40  1604.45  11966.049805  0.160258  0.162589   \n490      0.0     21099  1586.45  1586.45  11966.049805  0.160422  0.162755   \n\n      avg_div   gm_fact  gsd_fact  \n0    148.6915  0.000100       NaN  \n1    148.6915 -0.003277  0.006766  \n2    148.6915 -0.006281  0.006217  \n3    148.6915 -0.009058  0.005855  \n4    148.6915 -0.011168  0.005185  \n..        ...       ...       ...  \n486  148.6915  0.065673  0.008564  \n487  148.6915  0.065815  0.008569  \n488  148.6915  0.065953  0.008560  \n489  148.6915  0.066098  0.008553  \n490  148.6915  0.066239  0.008544  \n\n[491 rows x 18 columns]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "# Get square of log ratios\n",
    "log_square = df.close.rolling(2).apply(\n",
    "    lambda x: math.log(x[0]/x[-1])**2,\n",
    "    raw=True)\n",
    "\n",
    "# Sum the squares\n",
    "sum_log_square = log_square.expanding().sum()\n",
    "\n",
    "# Divide the sum with number of observations\n",
    "avg_sum_log_square = sum_log_square/sum_log_square.index\n",
    "\n",
    "# get the root of avg sum log square\n",
    "gsd_fact = avg_sum_log_square.apply(math.sqrt).apply(math.exp)-1\n",
    "\n",
    "# get the geometric mean\n",
    "gm = df.close.expanding().apply(gmean)\n",
    "\n",
    "# get the geometric mean difference from undPrice\n",
    "gm_fact = 1-gm/df.undPrice.unique()\n",
    "\n",
    "df = df.assign(gm_fact = gm_fact, gsd_fact = gsd_fact)\n",
    "\n",
    "# what we do with these gm and gsdd factors are unclear, especially with negative gm_facts\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n      <th>aMean</th>\n      <th>aSD</th>\n      <th>a_abv</th>\n      <th>a_blw</th>\n      <th>gMean</th>\n      <th>gSD</th>\n      <th>g_ucl</th>\n      <th>g_lcl</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.0</td>\n      <td>8.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.000000</td>\n      <td>1.000000</td>\n      <td>16.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.0</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>1.000000</td>\n      <td>16.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.0</td>\n      <td>8.333333</td>\n      <td>0.577350</td>\n      <td>8.910684</td>\n      <td>7.755983</td>\n      <td>8.320335</td>\n      <td>1.057094</td>\n      <td>17.115710</td>\n      <td>4.044704</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.0</td>\n      <td>8.500000</td>\n      <td>0.577350</td>\n      <td>9.077350</td>\n      <td>7.922650</td>\n      <td>8.485281</td>\n      <td>1.060660</td>\n      <td>17.485281</td>\n      <td>4.117749</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.0</td>\n      <td>8.400000</td>\n      <td>0.547723</td>\n      <td>8.947723</td>\n      <td>7.852277</td>\n      <td>8.385925</td>\n      <td>1.059399</td>\n      <td>17.269966</td>\n      <td>4.072026</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>11.0</td>\n      <td>9.520833</td>\n      <td>1.169608</td>\n      <td>10.690441</td>\n      <td>8.351226</td>\n      <td>9.448998</td>\n      <td>1.131470</td>\n      <td>20.140256</td>\n      <td>4.433090</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>11.0</td>\n      <td>9.536082</td>\n      <td>1.173153</td>\n      <td>10.709236</td>\n      <td>8.362929</td>\n      <td>9.463815</td>\n      <td>1.131829</td>\n      <td>20.175233</td>\n      <td>4.439295</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>10.0</td>\n      <td>9.540816</td>\n      <td>1.168031</td>\n      <td>10.708847</td>\n      <td>8.372785</td>\n      <td>9.469139</td>\n      <td>1.131253</td>\n      <td>20.181128</td>\n      <td>4.442992</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>8.0</td>\n      <td>9.525253</td>\n      <td>1.172329</td>\n      <td>10.697582</td>\n      <td>8.352923</td>\n      <td>9.453027</td>\n      <td>1.131850</td>\n      <td>20.152440</td>\n      <td>4.434188</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>11.0</td>\n      <td>9.540000</td>\n      <td>1.175679</td>\n      <td>10.715679</td>\n      <td>8.364321</td>\n      <td>9.467365</td>\n      <td>1.132188</td>\n      <td>20.186204</td>\n      <td>4.440210</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 9 columns</p>\n</div>",
      "text/plain": "    Value     aMean       aSD      a_abv     a_blw     gMean       gSD  \\\n0     8.0  8.000000       NaN        NaN       NaN  8.000000  1.000000   \n1     8.0  8.000000  0.000000   8.000000  8.000000  8.000000  1.000000   \n2     9.0  8.333333  0.577350   8.910684  7.755983  8.320335  1.057094   \n3     9.0  8.500000  0.577350   9.077350  7.922650  8.485281  1.060660   \n4     8.0  8.400000  0.547723   8.947723  7.852277  8.385925  1.059399   \n..    ...       ...       ...        ...       ...       ...       ...   \n95   11.0  9.520833  1.169608  10.690441  8.351226  9.448998  1.131470   \n96   11.0  9.536082  1.173153  10.709236  8.362929  9.463815  1.131829   \n97   10.0  9.540816  1.168031  10.708847  8.372785  9.469139  1.131253   \n98    8.0  9.525253  1.172329  10.697582  8.352923  9.453027  1.131850   \n99   11.0  9.540000  1.175679  10.715679  8.364321  9.467365  1.132188   \n\n        g_ucl     g_lcl  \n0   16.000000  4.000000  \n1   16.000000  4.000000  \n2   17.115710  4.044704  \n3   17.485281  4.117749  \n4   17.269966  4.072026  \n..        ...       ...  \n95  20.140256  4.433090  \n96  20.175233  4.439295  \n97  20.181128  4.442992  \n98  20.152440  4.434188  \n99  20.186204  4.440210  \n\n[100 rows x 9 columns]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this was made to test and build a case for computation with geometric mean\n",
    "# Failed Again\n",
    "# Refer to https://stats.stackexchange.com/questions/449482/geometric-standard-deviation-on-a-shifted-mean for stackoverflow question raised\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gstd\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "bmark = 10 # benchmark\n",
    "sdmult = 1 # standard deviation multiple\n",
    "\n",
    "data = pd.Series(np.random.randint(8, high=12, size=100), name='Value')\n",
    "\n",
    "am = pd.Series(data.expanding().mean(), name=\"aMean\") # arithmetic mean\n",
    "asd = pd.Series(data.expanding().std(ddof=1), name=\"aSD\") # arithmetic standard deviation\n",
    "a_abv = pd.Series(bmark-(bmark-am)+sdmult*asd, name=\"a_abv\") # benchmark value above multiple of sd\n",
    "a_blw = pd.Series(bmark-(bmark-am)-sdmult*asd, name=\"a_blw\") # benchmark value below multiple of sd\n",
    "\n",
    "gm_fact = pd.Series(data.expanding().apply(gmean), name=\"gMean\") # geometric mean factor\n",
    "gsd_fact = pd.Series(data.apply(math.log).expanding().apply(np.std).apply(math.exp), name=\"gSD\") # geometric standard deviation factor\n",
    "\n",
    "g_ucl = pd.Series((bmark-(bmark-gm_fact))*(math.sqrt(sdmult)+gsd_fact), name=\"g_ucl\") # geometric upper control limit\n",
    "g_lcl = pd.Series((bmark-(bmark-gm_fact))/(math.sqrt(sdmult)+gsd_fact), name=\"g_lcl\") # geometric lower control limit\n",
    "\n",
    "# computing above and below benchmark\n",
    "pd.DataFrame([data, am, asd, a_abv, a_blw, gm_fact, gsd_fact, g_ucl, g_lcl]).T"
   ]
  }
 ]
}